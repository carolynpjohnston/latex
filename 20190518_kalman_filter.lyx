#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Understanding the Kalman filter
\end_layout

\begin_layout Author
Carolyn Johnston
\end_layout

\begin_layout Section
Bayes Filters
\end_layout

\begin_layout Standard
This writeup follows the form of that in 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1"

\end_inset

, but adds some of the detail that the book leaves to the reader.
 
\end_layout

\begin_layout Standard
The Kalman filter is a special case of a Bayes filter.
 A Bayes filter is a recursive solution for estimating state probability
 density functions as they evolve over time, using a process model and measureme
nts.
 Using a Bayes filter, the state of a moving vehicle or robot (e.g.
 its position and orientation) can be tracked over time, using control data
 such as odometry, and incoming measurements from sensors such as lidar
 or cameras.
\end_layout

\begin_layout Standard
We start by assuming that the true state is a stochastic process: 
\begin_inset Formula $x_{0:T}=\{x_{0},...,x_{T}\},$
\end_inset

 where 
\begin_inset Formula $x_{t}$
\end_inset

 is the state vector at time 
\begin_inset Formula $t$
\end_inset

.
 Each transition 
\begin_inset Formula $x_{t-1}\rightarrow x_{t}$
\end_inset

 is driven by a (deterministic) control vector 
\begin_inset Formula $u_{t}$
\end_inset

.
 At each time step, a stochastic sensor measurement vector 
\begin_inset Formula $z_{t}$
\end_inset

 is collected.
 
\end_layout

\begin_layout Standard
For example, for the problem of tracking the location of a vehicle on a
 highway, 
\begin_inset Formula $x_{t}$
\end_inset

 might consist of its position vector relative to a fixed earth frame, together
 with an orientation (Euler angles, or a quaternion) relative to the frame.
 The vector 
\begin_inset Formula $u_{t}$
\end_inset

 could be odometry input from the vehicle's internal control system, and
 
\begin_inset Formula $z_{t}$
\end_inset

 might consist of GPS readings or lidar observations of landmarks (known
 or unknown).
 
\end_layout

\begin_layout Standard
The joint p.d.f.
 of the states and measurements is 
\begin_inset Formula $f(x_{0:T},z_{1:T}|u_{1:T}),$
\end_inset

 but we are interested in the p.d.f.
 of the final state conditioned on all previous states, and all measurements:
 
\begin_inset Formula $f(x_{T}|x_{0:T-1},z_{1:T},u_{1:T})$
\end_inset

.
 
\end_layout

\begin_layout Standard
The Bayes filter formulation assumes the state process is Markov.
 A Markov chain is a state process for which all future states are conditionally
 independent of past states and measurements, given the present state: it
 follows that
\begin_inset Formula 
\begin{equation}
f(x_{T}|x_{0:T-1},z_{1:T},u_{1:T})=f(x_{T}|x_{T-1},u_{T}).\label{eq:markov-transition}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
It is further assumed that
\begin_inset Formula 
\begin{equation}
f(z_{T}|x_{0:T},z_{1:T-1},u_{1:T})=f(z_{T}|x_{T}),\label{eq:markov-sensor}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
i.e.
 the measurement at time T is conditionally independent of all past states
 and data except the current state 
\begin_inset Formula $x_{T}.$
\end_inset


\end_layout

\begin_layout Standard
It follows that the system evolves according to a Hidden Markov Model.
 (
\emph on
Put graphical model here).
 
\end_layout

\begin_layout Standard
In other words, the joint p.d.f.
 simplifies to:
\begin_inset Formula 
\begin{equation}
f(x_{0},...,x_{T},z_{1},...,z_{T})=f(x_{0})\cdot\Pi_{t=1}^{T}f(x_{t}|x_{t-1})f(z_{t}|x_{t}).\label{eq:hmm}
\end{equation}

\end_inset


\end_layout

\begin_layout Section
Derivation of the general Bayes Filter algorithm
\end_layout

\begin_layout Standard
The goal of the Bayes filter algorithm is to calculate the 'belief' distribution
 at time 
\begin_inset Formula $T$
\end_inset

, 
\begin_inset Formula $bel(x_{T})=f(x_{T}|z_{1:T},u_{1:T})$
\end_inset

.
 This is the p.d.f.
 of the current state conditioned on all the data and measurements to that
 point, including the current sensor measurement 
\begin_inset Formula $z_{T}$
\end_inset

; in short, it is our probabilistic best guess of the system state at time
 
\begin_inset Formula $T.$
\end_inset

 Each Bayes filter recursion is a two step process.
 In the first 
\emph on
prediction
\emph default
 step, the control measurement 
\begin_inset Formula $u_{T}$
\end_inset

 is incorporated, and an intermediate belief 
\begin_inset Formula $\overline{bel}(x_{T})=f(x_{T}|z_{1:T-1},u_{1:T})$
\end_inset

, conditioned only on all past data and the current control measurement,
 is calculated.
 In the second 
\emph on
update 
\emph default
step, the sensor measurement 
\begin_inset Formula $z_{T}$
\end_inset

 is incorporated and the belief at time 
\begin_inset Formula $T$
\end_inset

 is calculated from 
\begin_inset Formula $\overline{bel}(x_{T}).$
\end_inset


\end_layout

\begin_layout Standard
In order to run the Bayes filter, we need the following:
\end_layout

\begin_layout Enumerate
The initial state probability density function, 
\begin_inset Formula $f(x_{0});$
\end_inset


\end_layout

\begin_layout Enumerate
Sensor measurements 
\begin_inset Formula $z_{t}$
\end_inset

 at each time 
\begin_inset Formula $t$
\end_inset

;
\end_layout

\begin_layout Enumerate
Control measurements 
\begin_inset Formula $u_{t}$
\end_inset

 at each time
\begin_inset Formula $t$
\end_inset

;
\end_layout

\begin_layout Enumerate
The p.d.f.
 of sensor measurements at each time 
\begin_inset Formula $t$
\end_inset

, conditioned on the state at time 
\begin_inset Formula $t$
\end_inset

, 
\begin_inset Formula $f(z_{t}|x_{t})$
\end_inset

 (this is called the 
\emph on
sensor model
\emph default
);
\end_layout

\begin_layout Enumerate
The p.d.f.
 of the state vector at each time 
\begin_inset Formula $t$
\end_inset

, conditioned on the state at time 
\begin_inset Formula $t-1$
\end_inset

 and the control measurement at time 
\begin_inset Formula $t$
\end_inset

, 
\begin_inset Formula $f(x_{t}|x_{t-1},u_{t})$
\end_inset

 (this is called the 
\emph on
process model
\emph default
).
\end_layout

\begin_layout Standard
The derivation proceeds by induction.
 First, we claim that 
\begin_inset Formula $bel(x_{0})=f(x_{0})$
\end_inset

; at time 
\begin_inset Formula $t=0$
\end_inset

 we have not yet seen any control or sensor measurements, so the belief
 is simply equal to the prior for 
\begin_inset Formula $x_{0}$
\end_inset

.
\end_layout

\begin_layout Standard
Next, assume that we are given 
\begin_inset Formula $f(x_{T-1}|z_{1:T-1},u_{1:T-1});$
\end_inset

 we will show how 
\begin_inset Formula $f(x_{T}|z_{1:T},u_{1:T})$
\end_inset

 is calculated using data 
\begin_inset Formula $u_{T}$
\end_inset

 and 
\begin_inset Formula $z_{T}$
\end_inset

, the sensor measurement model 
\begin_inset Formula $f(z_{T}|x_{T})$
\end_inset

, and the process transition model 
\begin_inset Formula $f(x_{T}|x_{T-1},u_{T}).$
\end_inset


\end_layout

\begin_layout Standard
We work backward from the final target distribution, 
\begin_inset Formula $bel(x_{T})=f(x_{T}|z_{1:T},u_{1:T})$
\end_inset

.
 By Bayes' theorem, applied to 
\begin_inset Formula $x_{T}$
\end_inset

 and 
\begin_inset Formula $z_{T}$
\end_inset

,
\begin_inset Formula 
\[
f(x_{T}|z_{1:T},u_{1:T})=f(x_{T}|z_{T},z_{1:T-1},u_{1:T})=\frac{f(z_{T}|x_{T},z_{1:T-1},u_{1:T})\cdot f(x_{T}|z_{1:T-1},u_{1:T})}{f(z_{T}|z_{1:T-1},u_{1:T})}.
\]

\end_inset


\end_layout

\begin_layout Standard
Notice that the denominator term is a constant with respect to 
\begin_inset Formula $x_{T}$
\end_inset

, so set 
\begin_inset Formula $\eta=1/f(z_{T}|z_{1:T-1},u_{1:T})$
\end_inset

 to get
\begin_inset Formula 
\[
f(x_{T}|z_{1:T},u_{1:T})=\eta\cdot f(z_{T}|x_{T},z_{1:T-1},u_{1:T})\cdot f(x_{T}|z_{1:T-1},u_{1:T}).
\]

\end_inset

From equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:markov-sensor"

\end_inset

, we have 
\begin_inset Formula 
\[
f(z_{T}|x_{T},z_{1:T-1},u_{1:T})=f(z_{T}|x_{T});
\]

\end_inset


\end_layout

\begin_layout Standard
and the second term, 
\begin_inset Formula $(x_{T}|z_{1:T-1},u_{1:T})$
\end_inset

, is equal to the intermediate belief 
\begin_inset Formula $\overline{bel}(x_{T}).$
\end_inset

 Thus the second step of the recursive Bayes filter formula isThis is true
 of the general Bayes filter algorithm; in the next section we will show
 how the Kalman filter is a special case of the Bayes filter, 
\begin_inset Formula 
\begin{equation}
f(x_{T}|z_{1:T},u_{1:T})=\eta\cdot f(z_{T}|x_{T})\cdot\overline{bel}(x_{T}).\label{eq:bayes-step1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Since we are given data 
\begin_inset Formula $z_{T}$
\end_inset

 and the sensor model 
\begin_inset Formula $f(z_{T}|x_{T}),$
\end_inset

 if we know 
\begin_inset Formula $\overline{bel}(x_{T})$
\end_inset

 then we can execute the second step.
 
\end_layout

\begin_layout Standard
In the first step of the Bayes filter, we calculate 
\begin_inset Formula 
\[
\overline{bel}(x_{T})=f(x_{T}|z_{1:T-1},u_{1:T})=f(x_{T}|u_{T},z_{1:T-1},u_{1:T}),
\]

\end_inset

 which incorporates the control measurement 
\begin_inset Formula $u_{T}.$
\end_inset


\end_layout

\begin_layout Standard
To do this, we note that
\begin_inset Formula 
\[
\overline{bel}(x_{T})=f(x_{T}|z_{1:T-1},u_{1:T})=\int f(x_{T},x_{T-1}|z_{1:T-1},u_{1:T})dx_{T-1};
\]

\end_inset

i.e., we re-introduce the previous state 
\begin_inset Formula $x_{T-1}$
\end_inset

 by marginalizing over it.
 We have:
\begin_inset Formula 
\[
\overline{bel}(x_{T})=\int f(x_{T},x_{T-1}|z_{1:T-1},u_{1:T})dx_{T-1}
\]

\end_inset


\begin_inset Formula 
\[
=\int f(x_{T}|x_{T-1},z_{1:T-1},u_{1:T})\cdot f(x_{T-1}|z_{1:T-1},u_{1:T})dx_{T-1}.
\]

\end_inset


\end_layout

\begin_layout Standard
The first term in the integrand simplifies according to equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:markov-transition"

\end_inset

.
 Note that the second term in the integrand satisfies
\begin_inset Formula 
\[
f(x_{T-1}|z_{1:T-1},u_{1:T})=f(x_{T-1}|z_{1:T-1},u_{1:T-1}),
\]

\end_inset


\end_layout

\begin_layout Standard
i.e., the p.d.f.
 for 
\begin_inset Formula $x_{T-1}$
\end_inset

at time step 
\begin_inset Formula $T-1$
\end_inset

 is independent of 
\begin_inset Formula $u_{T},$
\end_inset

 the control at time
\begin_inset Formula $T$
\end_inset

.
 Thus the second term in the integrand is equal to 
\begin_inset Formula $bel(x_{T-1})$
\end_inset

, and we have
\begin_inset Formula 
\begin{equation}
\overline{bel}(x_{T})=\int f(x_{T}|x_{T-1},u_{T})\cdot bel(x_{T-1})dx_{T-1}.\label{eq:bayes-step2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Equations 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:bayes-step1"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:bayes-step2"

\end_inset

 form the Bayes filter algorithm to transition from 
\begin_inset Formula $bel(x_{T-1})$
\end_inset

 to 
\begin_inset Formula $bel(x_{T}).$
\end_inset

 
\end_layout

\begin_layout Standard
The first step of the Bayes filter algorithm, calculating 
\begin_inset Formula $\overline{bel}(x_{T}),$
\end_inset

 is essentially a convolution of the previous belief with the transition
 p.d.f.
 in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:markov-transition"

\end_inset

; the previous belief is essentially smeared by the transition p.d.f., and
 so 
\begin_inset Formula $\overline{bel}(x_{T})$
\end_inset

 has increased uncertainty relative to 
\begin_inset Formula $bel(x_{T-1})$
\end_inset

.
 The second step of the Bayes filter algorithm multiplies 
\begin_inset Formula $\overline{bel}(x_{T})$
\end_inset

 by the likelihood of 
\begin_inset Formula $x_{T}$
\end_inset

 after the measurement 
\begin_inset Formula $z_{T}$
\end_inset

, so this step reduces the uncertainty of 
\begin_inset Formula $\overline{bel}(x_{T})$
\end_inset

 in accordance with the sensor measurement.
 
\end_layout

\begin_layout Section
The Kalman filter
\end_layout

\begin_layout Standard
The Kalman filter is the Bayes filter that is derived when the following
 assumptions are made:
\end_layout

\begin_layout Enumerate
The prior 
\begin_inset Formula $f(x_{0})$
\end_inset

 is a multivariate Gaussian in the state vector 
\begin_inset Formula $x_{0}$
\end_inset

;
\end_layout

\begin_layout Enumerate
For each time step, the state transition equation must be linear in its
 arguments 
\begin_inset Formula $x_{t-1}$
\end_inset

 and 
\begin_inset Formula $u_{t}$
\end_inset

, with additive mean-zero Gaussian noise 
\begin_inset Formula $\epsilon_{t}$
\end_inset

:
\begin_inset Formula 
\begin{equation}
x_{t}=A_{t}x_{t-1}+B_{t}u_{u}+\epsilon_{t}\label{eq:kalman-transition}
\end{equation}

\end_inset


\end_layout

\begin_layout Enumerate
For each time step, the sensor equation must be linear in its argument 
\begin_inset Formula $x_{t}$
\end_inset

 with additive mean-zero Gaussian noise 
\begin_inset Formula $\delta_{t}:$
\end_inset


\begin_inset Formula 
\begin{equation}
z_{t}=C_{t}x_{t}+\delta_{t}.\label{eq:kalman-sensor}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $A_{t},B_{t},C_{t}$
\end_inset

 are all linear (matrices), and their values (as well as the parameters
 of the noise vectors 
\begin_inset Formula $\epsilon_{t}$
\end_inset

 and 
\begin_inset Formula $\delta_{t}$
\end_inset

) can vary with each time step.
 These conditions ensure that the beliefs 
\begin_inset Formula $bel(x_{t})=f(x_{t}|z_{1:T},u_{1:T})$
\end_inset

 are multivariate Gaussian distributions, which we will show.
 
\end_layout

\begin_layout Standard
It follows from equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:kalman-transition"

\end_inset

 that the transition model p.d.f.
 is given by
\begin_inset Formula 
\[
f(x_{t}|x_{t-1},u_{t})\propto\text{\text{exp}}(\frac{-(x_{t}-A_{t}x_{t-1}-B_{t}u_{t})^{T}R_{t}^{-1}(x_{t}-A_{t}x_{t-1}-B_{t}u_{t})}{2}),
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $R_{t}$
\end_inset

 is the covariance of 
\begin_inset Formula $\epsilon_{t}.$
\end_inset

 Similarly, it follows from equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:kalman-sensor"

\end_inset

 that the sensor model p.d.f.
 is given by
\begin_inset Formula 
\[
f(z_{t}|x_{t})\propto\text{exp}(\frac{-(z_{t}-C_{t}x_{t})^{T}Q_{t}^{-1}(z_{t}-C_{t}x_{t})}{2}),
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $Q_{t}$
\end_inset

 is the covariance of 
\begin_inset Formula $\delta_{t}.$
\end_inset


\end_layout

\begin_layout Section
Derivation of the prediction step of the Kalman filter
\end_layout

\begin_layout Standard
The Kalman filter, as a special case of the Bayes filter, has two steps,
 mirroring the prediction equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:bayes-step2"

\end_inset

 and the sensor update equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:bayes-step1"

\end_inset

.
 We prove that each belief 
\begin_inset Formula $bel(x_{t})$
\end_inset

 is Gaussian, and derive the equations, by induction on 
\begin_inset Formula $t$
\end_inset

.
 
\end_layout

\begin_layout Standard
We are given that 
\begin_inset Formula $bel(x_{0})$
\end_inset

 is Gaussian, with 
\begin_inset Formula $bel(x_{0})\sim N(\mu_{0},\Sigma_{0}).$
\end_inset

 
\end_layout

\begin_layout Standard
Now, suppose that 
\begin_inset Formula $bel(x_{t-1})$
\end_inset

 is Gaussian, with 
\begin_inset Formula $bel(x_{t-1})\sim N(\mu_{t-1},\Sigma_{t-1})$
\end_inset

.
 From 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:bayes-step2"

\end_inset

, we have that the intermediate belief is
\begin_inset Formula 
\[
\overline{bel}(x_{t})=\int f(x_{t}|x_{t-1},u_{t})\cdot bel(x_{t-1})dx_{t-1}\propto
\]

\end_inset


\begin_inset Formula 
\begin{equation}
\int\text{exp}(-\frac{1}{2}[(x_{t}-A_{t}x_{t-1}-B_{t}u_{t})^{T}R_{t}^{-1}(x_{t}-A_{t}x_{t-1}-B_{t}u_{t})+(x_{t-1}-\mu_{t-1})^{T}\Sigma_{t-1}^{-1}(x_{t-1}-\mu_{t-1})])dx_{t-1}.\label{eq:kalman-prediction-step1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The result for the prediction step follows from the convolution theorem
 for Gaussians, proven at the end of this article, with substitutions 
\begin_inset Formula $x=x_{t}$
\end_inset

, 
\begin_inset Formula $y=x_{t-1},$
\end_inset

 
\begin_inset Formula $A=A_{t},$
\end_inset

 
\begin_inset Formula $w=B_{t}u_{t}$
\end_inset

,
\begin_inset Formula $\mu=\mu_{t-1}$
\end_inset

, 
\begin_inset Formula $R=R_{t},$
\end_inset

 and 
\begin_inset Formula $\Sigma=\Sigma_{t-1}.$
\end_inset

 
\begin_inset Formula 
\[
\overline{bel}(x_{t})\sim N(A_{t}\mu_{t-1}+B_{t}u_{t},A_{t}\Sigma_{t-1}A_{t}^{T}+R_{t}).
\]

\end_inset


\end_layout

\begin_layout Standard
Note that where the covariance of the Gaussian 
\begin_inset Formula $bel(x_{t-1})$
\end_inset

 was 
\begin_inset Formula $\Sigma_{t-1},$
\end_inset

 that of 
\begin_inset Formula $\overline{bel}(x_{t})$
\end_inset

 is 
\begin_inset Formula $A_{t}\Sigma_{t-1}A_{t}^{T}+R_{t}.$
\end_inset

 Essentially, we have propagated the uncertainty in the state 
\begin_inset Formula $x_{t-1}$
\end_inset

through the linear function 
\begin_inset Formula $A_{t},$
\end_inset

 and added the process noise from 
\begin_inset Formula $\epsilon_{t}.$
\end_inset


\end_layout

\begin_layout Section
Derivation of the update step of the Kalman filter
\end_layout

\begin_layout Section
A convolution theorem for Gaussians
\end_layout

\begin_layout Standard
The following convolution identity was used in the derivation of the Kalman
 filter and will be derived here:
\end_layout

\begin_layout Quote
\begin_inset Formula 
\[
f(x)=\int\text{exp}(-\frac{1}{2}[(x-Ay-w)^{T}R^{-1}(x-Ay-w)+(y-\mu)^{T}\Sigma^{-1}(y-\mu)]dy
\]

\end_inset


\begin_inset Formula 
\[
\implies f(x)\sim N(A\mu+w,A\Sigma A^{T}+R).
\]

\end_inset

 
\series bold
Proof
\series default
.
 The proof proceeds by expressing 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
the quadratic argument of the exponential in the integrand, 
\begin_inset Formula 
\[
k(x,y)=(x-Ay-w)^{T}R^{-1}(x-Ay-w)+(y-\mu)^{T}\Sigma^{-1}(y-\mu),
\]

\end_inset

, as:
\begin_inset Formula 
\begin{equation}
k(x,y)=(y-u(x))^{T}T^{-1}(y-u(x))+\beta(x),\label{eq:k_xy}
\end{equation}

\end_inset


\end_layout

\begin_layout Quote
where 
\begin_inset Formula $v\mbox{ }$
\end_inset

is a constant with respect to both 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y$
\end_inset

, 
\begin_inset Formula $T$
\end_inset

 is some covariance matrix, 
\begin_inset Formula $u(x)$
\end_inset

 is linear in 
\begin_inset Formula $x$
\end_inset

, and 
\begin_inset Formula $\beta$
\end_inset

 is independent of 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\begin_layout Quote

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
Then 
\begin_inset Formula 
\[
\int\text{exp}(-\frac{1}{2}[(y-u(x))^{T}T^{-1}(y-u(x))+\beta(x)])dy=
\]

\end_inset


\begin_inset Formula 
\[
\text{exp}(-\frac{1}{2}[\beta(x))\cdot\int\mbox{\text{exp(-\frac{1}{2}[}}(y-u(x))^{T}T^{-1}(y-u(x))])dy
\]

\end_inset


\begin_inset Formula 
\[
\propto\text{exp}(-\frac{1}{2}[\beta(x)]),
\]

\end_inset

since the integral is a constant multiple of a p.d.f.
 in 
\begin_inset Formula $y$
\end_inset

.
 We will also show that the function 
\begin_inset Formula $\beta(x)$
\end_inset

 is quadratic in 
\begin_inset Formula $x$
\end_inset

 alone: 
\begin_inset Formula $\beta(x)=(x-v)^{T}L^{-1}(x-v)$
\end_inset

 for some 
\begin_inset Formula $L,$
\end_inset

 where 
\begin_inset Formula $v$
\end_inset

 is constant with respect to both 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $y;$
\end_inset

 in fact, we will see that 
\begin_inset Formula $v=A\mu+w,$
\end_inset

 and 
\begin_inset Formula $L=A\Sigma A^{T}+R.$
\end_inset

 
\end_layout

\begin_layout Quote
We proceed by finding a quadratic function, 
\begin_inset Formula $\alpha(y)$
\end_inset

, such that the first and second derivatives of 
\begin_inset Formula $\alpha(y)$
\end_inset

 are equal to those of 
\begin_inset Formula $k(x,y).$
\end_inset

 Since 
\begin_inset Formula $k(x,y)$
\end_inset

 is quadratic in 
\begin_inset Formula $y$
\end_inset

, it follows that all the 
\begin_inset Formula $y$
\end_inset

-derivatives of 
\begin_inset Formula $\alpha$
\end_inset

 and 
\begin_inset Formula $k$
\end_inset

 are equal, and therefore 
\begin_inset Formula $\beta(x)=k(x,y)-\alpha(y)$
\end_inset

 is constant with respect to 
\begin_inset Formula $y.$
\end_inset


\end_layout

\begin_layout Quote
We have 
\begin_inset Formula 
\begin{equation}
\frac{dk(x,y)}{dy}=-2[A^{T}R^{-1}(x-Ay-w)+\Sigma^{-1}(y-\mu)]\label{eq:dk_dy}
\end{equation}

\end_inset


\end_layout

\begin_layout Quote
and 
\begin_inset Formula 
\[
\frac{d^{2}k(x,y)}{dy^{2}}=-2A^{T}R^{-1}A-2\Sigma^{-1}.
\]

\end_inset


\end_layout

\begin_layout Quote
Solving 
\begin_inset Formula $\frac{dk(x,y)}{dy}=0$
\end_inset

 for the constant term
\begin_inset Formula $u(x)$
\end_inset

 gives
\begin_inset Formula 
\[
(A^{T}R^{-1}A+\Sigma^{-1})y_{0}=\Sigma^{-1}\mu+A^{T}R^{-1}(x-w),
\]

\end_inset

 or
\begin_inset Formula 
\[
y_{0}=u(x)=(A^{T}R^{-1}A+\Sigma^{-1})^{-1}(\Sigma^{-1}\mu+A^{T}R^{-1}(x-w)).
\]

\end_inset


\end_layout

\begin_layout Quote
Therefore, defining
\begin_inset Formula 
\[
\alpha(y)=(y-u(x))^{T}T^{-1}(y-u(x))=(y-y_{0})^{T}(A^{T}R^{-1}A+\Sigma^{-1})(y-y_{0})
\]

\end_inset

guarantees that 
\begin_inset Formula $\beta(x)=k(x,y)-\alpha(y)$
\end_inset

 is constant with respect to 
\begin_inset Formula $y$
\end_inset

.
 For computational convenience, we'll define
\begin_inset Formula 
\[
P\equiv(A^{T}R^{-1}A+\Sigma^{-1})
\]

\end_inset


\end_layout

\begin_layout Quote
in what follows, so that 
\begin_inset Formula $y_{0}=u(x)=P^{-1}(\Sigma^{-1}\mu+A^{T}R^{-1}(x-w))$
\end_inset

 and
\begin_inset Formula 
\[
\alpha(y)=(y-y_{0})^{T}P(y-y_{0}).
\]

\end_inset


\end_layout

\begin_layout Quote
Next, we must find 
\begin_inset Formula $\beta(x)$
\end_inset

.
 We will begin by calculating 
\begin_inset Formula $\frac{d\beta}{dx}$
\end_inset

 and 
\begin_inset Formula $\frac{d\beta^{2}}{dx^{2}}$
\end_inset

, and will see that 
\begin_inset Formula $\frac{d\beta^{2}}{dx^{2}}$
\end_inset

 is constant with respect to 
\begin_inset Formula $x;$
\end_inset

 therefore, 
\begin_inset Formula $\beta(x)$
\end_inset

 is a quadratic function of 
\begin_inset Formula $x.$
\end_inset

 
\end_layout

\begin_layout Quote
We have 
\begin_inset Formula 
\[
\beta\equiv k(x,y)-\alpha(y)=(x-Ay-w)^{T}R^{-1}(x-Ay-w)
\]

\end_inset


\begin_inset Formula 
\[
+(y-\mu)^{T}\Sigma^{-1}(y-\mu)-(y-u(x))^{T}P(y-u(x)).
\]

\end_inset


\end_layout

\begin_layout Quote
Since we have shown that 
\begin_inset Formula $\beta$
\end_inset

 is independent of 
\begin_inset Formula $y$
\end_inset

, we can remove the explicit dependency on 
\begin_inset Formula $y$
\end_inset

 by setting 
\begin_inset Formula $y\equiv0$
\end_inset

, which gives:
\begin_inset Formula 
\[
\beta=(x-w)^{T}R^{-1}(x-w)+\mu^{T}\Sigma^{-1}\mu-(u(x)^{T}Pu(x)).
\]

\end_inset


\end_layout

\begin_layout Quote
Taking the first derivative with respect to 
\begin_inset Formula $x$
\end_inset

 gives:
\begin_inset Formula 
\[
\frac{d\beta}{dx}=2R^{-1}(x-w)-2(A^{T}R^{-1})^{T}P^{-1}(P)P^{-1}(\Sigma^{-1}\mu+A^{T}R^{-1}(x-w))
\]

\end_inset


\begin_inset Formula 
\[
=2R^{-1}(x-w)-2R^{-1}AP^{-1}(\Sigma^{-1}\mu+A^{T}R^{-1}(x-w)).
\]

\end_inset


\end_layout

\begin_layout Quote
Setting 
\begin_inset Formula $\frac{d\beta}{dx}=0$
\end_inset

 to get the extremum of 
\begin_inset Formula $\beta$
\end_inset

 gives
\begin_inset Formula 
\[
(R^{-1}-R^{-1}AP^{-1}A^{T}R^{-1})(x-w)=R^{-1}AP^{-1}\Sigma^{-1}\mu.
\]

\end_inset


\end_layout

\begin_layout Quote
Fortunately, by the Woodbury formula, we have 
\begin_inset Formula 
\[
R^{-1}-R^{-1}AP^{-1}A^{T}R^{-1}=R^{-1}-R^{-1}A(A^{T}R^{-1}A+\Sigma^{-1})^{-1}A^{T}R^{-1}
\]

\end_inset


\begin_inset Formula 
\[
=(R+A\Sigma A^{T})^{-1},
\]

\end_inset


\end_layout

\begin_layout Quote
so that 
\begin_inset Formula 
\[
(R+A\Sigma A^{T})^{-1}(x-w)=R^{-1}A(A^{T}R^{-1}A+\Sigma^{-1})^{-1}\Sigma^{-1}\mu.
\]

\end_inset


\end_layout

\begin_layout Quote
It follows that 
\begin_inset Formula 
\begin{equation}
x=w+(R+A\Sigma A^{T})R^{-1}A(A^{T}R^{-1}A+\Sigma^{-1})^{-1}\Sigma^{-1}\mu.\label{eq:mess}
\end{equation}

\end_inset

Fortunately, more simplification is possible.
 We have
\begin_inset Formula 
\[
(A^{T}R^{-1}A+\Sigma^{-1})^{-1}\Sigma^{-1}=(\Sigma A^{T}R^{-1}A+I)^{-1},
\]

\end_inset


\end_layout

\begin_layout Quote
and
\begin_inset Formula 
\[
(R+A\Sigma A^{T})R^{-1}A=(A+A\Sigma A^{T}R^{-1}A)=A(I+\Sigma A^{T}R^{-1}A).
\]

\end_inset


\end_layout

\begin_layout Quote
Making these substitutions in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:mess"

\end_inset

 gives
\begin_inset Formula 
\[
x=w+(R+A\Sigma A^{T})R^{-1}A(A^{T}R^{-1}A+\Sigma^{-1})^{-1}\Sigma^{-1}\mu
\]

\end_inset


\begin_inset Formula 
\[
=w+A(I+\Sigma A^{T}R^{-1}A)(I+\Sigma A^{T}R^{-1}A)^{-1}\mu=w+A\mu.
\]

\end_inset


\end_layout

\begin_layout Quote
Thus, the extremum of the function 
\begin_inset Formula $\beta(x)$
\end_inset

 is the very simple expression 
\begin_inset Formula $x_{0}=w+A\mu.$
\end_inset


\end_layout

\begin_layout Quote
To establish the quadratic function 
\begin_inset Formula $\beta$
\end_inset

, we still need 
\begin_inset Formula $\frac{d^{2}\beta}{d^{2}x}.$
\end_inset


\begin_inset Formula 
\[
\frac{d^{2}\beta}{dx^{2}}=\frac{d}{dx}(2R^{-1}(x-w)-2R^{-1}AP^{-1}(\Sigma^{-1}\mu+A^{T}R^{-1}(x-w)))
\]

\end_inset


\begin_inset Formula 
\[
=2(R^{-1}-2R^{-1}AP^{-1}A^{T}R^{-1})=2(R+A\Sigma A^{T})^{-1},
\]

\end_inset


\end_layout

\begin_layout Quote
again using the Woodbury identity.
 Since the second derivative is a constant with respect to 
\begin_inset Formula $x$
\end_inset

, 
\begin_inset Formula $\beta(x)$
\end_inset

 is a quadratic function which must be of the form 
\begin_inset Formula $N(w+A\mu,R+A\Sigma A^{T}).$
\end_inset

 This completes the proof of the convolution identity.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "1"
key "key-1"

\end_inset

Thrun, S., Burgard, W., Fox, D.
 
\emph on
Probabilistic Robotics.
 (2006)
\end_layout

\end_body
\end_document
